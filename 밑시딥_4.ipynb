{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ff8123",
   "metadata": {},
   "source": [
    "신경망 구현을 위해서 활성함수 사용\n",
    "- 계단함수, 시그모이드 함수 등을 사용함\n",
    "- 차이 \n",
    "    + 계단함수 : 임계값을 넘지 못하면 0, 임계값을 넘으면 1을 출력하는 함수\n",
    "    + 시그모이드 : 출력값이 0~1 사이로 출력하는 함수\n",
    "    + \n",
    "- 활성화 함수 : 어떤 노드를 활성화 할지 결정하는 것. \n",
    "    -> 활성화 함수는 무조건 값을 출력하는데, 이 값을 다음 노드(뉴런)으로 전달할지 말지 정할 수도 있음(DropOut이라는 기법을 통해서)  \n",
    " -   **활성화함수는 노드의 활성화 여부를 결정하는 함수가 아니고, 가중치의 활성화 여부를 결정하는 함수**다.......  \n",
    "    \n",
    " -   **가중치를 얼마만큼 받아드릴지 결정하는 것. 활성화 함수의 출력값이 곧 다음 뉴런에 전달할 가중치 값**이다.   \n",
    "    \n",
    "   \n",
    "    - 활성화 함수는 모든 노드안에 있음. 그래서 시그모이드를 거치면 가중치 값이 작아지는 것이 문제가 되는 것. -> 가중치가 1000이었어도 1이 되는 것. -> 렐루를 쓰면 그 가중치 값이 그대로 전달되는 것.   \n",
    "    - h(a) -> y 값이 나오면 그 y값이 다음 노드의 가중치 값으로 출력됨. -> 다음 노드에서 활성화함수의 x값으로 들어가게 됨. \n",
    "- **모든 노드는 출력값이 있고, 다음 노드의 가중치로 출력이 됨. 그 값을 크게줄지 적게 줄지 결정하는 것**\n",
    "\n",
    "- 시그모이드 tanh 차이는 그 기울기 제일 높은 값이 0.0에 있느냐(tanh)없느냐 차이.\n",
    "- 시그모이드함수보다 ReLU가 더 좋을까? 경우에 따라 사용방법이 다르지만, 시그모이드는 기울기 소실이 되는 문제점이 있고, ReLU는 기울기 소실이 없음....\n",
    "\n",
    "- 선형함수와 은닉층이 없는 네트워크? 비선형함수를 나타내기위해 레이어를 쌓다보니 은닉층이 생김...! 신경망으로 갈수록 비선형 -> 비선형이 \n",
    "- 은닉층이 없으면 단일함수 -> 단층 퍼셉트론. 은닉층이 있어야 신경망이 됨\n",
    "- 회귀에는 항등함수, 분류에는 시그모이드, 다중 클래스 분류에는 소프트맥스를 사용함 \n",
    "- 회귀(몇개의 변수를 이용해 가설을 세우고 가설을 확인하는..?)는 선형으로 나타낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654daa3",
   "metadata": {},
   "source": [
    "활성화 함수의 기능? 해당 노드가 네트워크상에서 얼마나 활성화를 시킬지를 결정함. 네트워크는 가중치가 높은 노드를 최우선으로 사용함. 이 노드가 얼마나 중요한지 어필하는 함수!!\n",
    "- 활성화함수는 1000개가넘는다~\n",
    "\n",
    "    + 시그모이드 : 값을 0~1로 출력하는데, 이를 비선형으로 표현하기 위해서 사용. -> 정규화 되다보니 가중치가 계속 0~1사이값으로 고정됨. -> 탄젠트h가 나옴. -> -1 ~ 1사이 값. -1일수록 강한 부정. 예전엔 활성/비활성만 했는데, 탄젠트는 강한부정/중립/강한부정으로 표현 가능. -> 표현의 가지수가 달라짐. \n",
    "    + tanh도 정규화가 되니까 가중치가 소실됨 -> 렐루\n",
    "    + ReLU -> 0 ~ x 값으로 방출. ㅇ데이터 정규화 처리하지 않음. \n",
    "    + ReLU는 0이하값이 없으니 보완하기 위해 Leakly ReLU, Paramitic ReLU가 나옴. \n",
    "    + Maxout 큰값만 출력하자! \n",
    "    \n",
    "    -> 어떤 노드를 중요하게 볼지 기준을 판단해주는 것. \n",
    "    데이터를 어떻게 구분할 것인가. 나누는 것. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f0c41",
   "metadata": {},
   "source": [
    "- 소프트맥스는? 각클래스마다 지수함수를 통해 해당 클래스의 확률을 구함 00> 이 클래스가 정답에 속할 확률에 따라 확률이 노픙ㄴ 값을 정답이라고 말하는 것!\n",
    "- 원핫인코딩이란? 목적인 값만 1로 표현하는 방식 -> 배열의 형태로 정리하는 것 (임베딩)\n",
    "- 배치 ? 큰싸이즈의 배열을 한번에 계산하기 위해서// 한번 훈련할때 몇개의 데이터를 사용할 것인가? 배치가 크면 전체적으로 보는것 -> 업데이트 값이 큼. 배치사이즈는 가중치를 조금씩 여러번 업데이트하는 방식. 배치가 크면 신경망이 데이터의 전체적인 특징을 알 수 있음. 단점은 세심한 학습을 못함. 다봐야하니까. 반대로 작으면 세심한 특징을 찾는 것은 잘함. 세심하게 다보니 전체적인 흐름을 놓침. \n",
    "- 정규화 : 데이터를 목적에 맞게 일정한 범위의 값으로 치환하는 것 .\n",
    "- 전처리 : 데이터를 모델의 목적에 맞게 다듬는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5eb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
